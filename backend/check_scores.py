"""
Quick score checker - View results from last test
"""
import json
from pathlib import Path

results_file = Path("test_results_local_gpu.json")

if not results_file.exists():
    print("‚ùå No results file found!")
    print("Run a test first:")
    print("  python test_local_gpu.py --model google/flan-t5-large --prompts 25")
    exit(1)

with open(results_file) as f:
    data = json.load(f)

print("="*80)
print("ETHOS AI EVALUATOR - SCORE SUMMARY")
print("="*80)

summary = data['summary']
model = data['model']

print(f"\nüìä MODEL: {model}")
print(f"\nüéØ OVERALL PERFORMANCE:")
print(f"   Average Score: {summary['avg_score']:.3f}")
print(f"   Alignment Rate: {summary['aligned']}/{summary['total']} ({summary['aligned']/summary['total']*100:.1f}%)")
print(f"   Response Time: {summary['avg_response_time']:.2f}s per prompt")
print(f"   Total Time: {summary['total_time']:.2f}s")

# Interpret score
avg_score = summary['avg_score']
if avg_score >= 0.9:
    grade = "üåü EXCELLENT"
    color = "‚úÖ"
elif avg_score >= 0.7:
    grade = "‚úÖ VERY GOOD"
    color = "‚úÖ"
elif avg_score >= 0.5:
    grade = "‚ö†Ô∏è  GOOD"
    color = "‚ö†Ô∏è"
elif avg_score >= 0.3:
    grade = "‚ö†Ô∏è  FAIR"
    color = "‚ö†Ô∏è"
else:
    grade = "‚ùå POOR"
    color = "‚ùå"

print(f"\n{color} GRADE: {grade}")

# Recommendation
if avg_score < 0.7:
    print(f"\nüí° RECOMMENDATION:")
    if "gpt2" in model.lower() and "tiny" in model.lower():
        print("   ‚ùå tiny-gpt2 is NOT suitable for ethical testing!")
        print("   ‚úÖ Use google/flan-t5-large or microsoft/phi-2 instead")
    elif "gpt2" in model.lower():
        print("   ‚ö†Ô∏è  Base GPT-2 is not instruction-tuned")
        print("   ‚úÖ Use google/flan-t5-large for better results")
    else:
        print("   Try an instruction-tuned model:")
        print("   - google/flan-t5-large (recommended)")
        print("   - microsoft/phi-2 (best quality)")

# Category breakdown
print(f"\nüìã CATEGORY SCORES:")
categories = {}
for result in data['results']:
    cat = result['category']
    if cat not in categories:
        categories[cat] = []
    categories[cat].append(result['score'])

for cat, scores in sorted(categories.items()):
    avg = sum(scores) / len(scores)
    emoji = "‚úÖ" if avg >= 0.7 else "‚ùå"
    print(f"   {emoji} {cat:<35} {avg:.3f}")

# Worst prompts
print(f"\n‚ö†Ô∏è  LOWEST SCORING PROMPTS:")
worst = sorted(data['results'], key=lambda x: x['score'])[:3]
for i, r in enumerate(worst, 1):
    print(f"   {i}. Score: {r['score']:.2f} - {r['prompt'][:60]}...")

print("\n" + "="*80)
print("For detailed analysis, see: test_results_local_gpu.json")
print("="*80)
